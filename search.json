[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Reinforcement Learning Quarto",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "Tarea 2.html",
    "href": "Tarea 2.html",
    "title": "2  Tarea 1",
    "section": "",
    "text": "2.1 Exercise 1\nExercise 1 Read (Sec 1.1, pp 1-2 ) and answer the following. Explain why Reinforcement Learning differs for supervised and unsupervised learning.\nEl aprendizaje reforzado se centra en aprender a qué hacer y cómo hacer situaciones que lleven a maximizar una recompensa, por otro lado el aprendizaje supervisado consiste en aprender de un conjuto de entrenamiento de ejemplos etiquetados dados por un supervisor externo con conocimientos. El objetivo de este aprendizaje es que el sistema extrapole o generalice sus respuestas para actuar correctamente en situaciones que no están presentes en el conjunto de entrenamiento.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Tarea 1</span>"
    ]
  },
  {
    "objectID": "Tarea 2.html#exercise-2",
    "href": "Tarea 2.html#exercise-2",
    "title": "2  Tarea 1",
    "section": "2.2 Exercise 2",
    "text": "2.2 Exercise 2\nExercise 2 See the first Steve Bruton’s youtube video about Reinforcement Learning. Then accordingly to its presentation explain what is the meaning of the following expression: \\[V_{\\pi}(s)=E\\left(\\sum_{t}\\gamma^tr_t|s_0=s \\right)\\]\nLa función mide de una forma que tan buenas son las acciones que se eligen, es decir la función de valor como la recompensa esperada habiendo elegido una política y un estado inicial.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Tarea 1</span>"
    ]
  },
  {
    "objectID": "Tarea 2.html#exercise-3",
    "href": "Tarea 2.html#exercise-3",
    "title": "2  Tarea 1",
    "section": "2.3 Exercise 3",
    "text": "2.3 Exercise 3\nExercise 3 Form (see ) obtain a time line pear year from 1950 to 2012.\n\nlibrary(bibtex)\n## Activate the Core Packages\nlibrary(tidyverse) ## Brings in a core of useful functions\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\nlibrary(gt)        ## Tables\n## Specific packages\nlibrary(milestones)\n## Initialize defaults\n## Initialize defaults\ncolumn &lt;- lolli_styles()\n\ndata &lt;- read_csv(col_names=TRUE, show_col_types=FALSE, file='rl_time_line.csv')\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\n\n## Sort the table by date\ndata &lt;- data |&gt;\n  arrange(date)\n\n## Build a table\ngt(data) |&gt;\n  #cols_hide(columns = event) |&gt;\n  tab_style(cell_text(v_align = \"top\"),\n            locations = cells_body(columns = date)) |&gt;\n  tab_source_note(source_note = \"Source: Sutton and Barto (2018)\") \n\n\n\n\n\n\n\ndate\nevent\nreference\n\n\n\n\n1911\nPrimer idea del \"trial-and-error learning\" (TaEL)\nNA\n\n\n1927\nAparece el termino \"Reinforcement\" en el contexto del aprendizaje animal\nNA\n\n\n1948\nTuring describe un diseno para un sistema \"pleasure-pain system\"\nNA\n\n\n1954\nMinsky Farley y Clark publican sus investigaciones sobre TaEL.\nNA\n\n\n1957\nAparece \"Dynamic Programming\" (DP)\nBellman, R. E. (1957a). Dynamic Programming. Princeton University Press,Princeton.\n\n\n1957\nIntroducen los Markov Decision Processes (MDP)\nBellman, R. E. (1957b). A Markov decision process. Journal of Mathematical Mechanics, 6:679–684\n\n\n1959\nComienza a utilizarse el \"optimal control\" (OC)\nNA\n\n\n1959\nSe desarrolla extensamente la DP\nNA\n\n\n1960\nAparece el metodo de iteracion de politicas\nHoward, R. (1960). Dynamic Programming and Markov Processes. MIT Press, Cambridge, MA.\n\n\n1960\nSe utilizan por primera vez en ingeniería\"Reinforcement\" y \"Reinforcement Learning\"\nNA\n\n\n1960\nSe originan los \"Learning Automata\"\nNA\n\n\n1961\nSe publica \"Steps Toward Artificial Intelligence\"\nNA\n\n\n1961\nSe describe un sistema \"TaEL\" para el tic-tac-toe\nNA\n\n\n1963\nSe desarrolla STeLLA\nNA\n\n\n1972\nTrabajo de Klopf\nNA\n\n\n1973\nWidrow, Gupta y Maitra modifican el LMS.\nNA\n\n\n1973\nTeoría del aprendizaje de Bush y Mostelle\nNA\n\n\n1977\nWerbos conecta el OC y PD\nWerbos, P. J. (1977). Advanced forecasting methods for global crisis warning and models of intelligence. General Systems Yearbook, 22:25–38.\n\n\n1978\nSutton desarrolla las ideas de Klopf\nNA\n\n\n1986\nIntroducen los clasificadores\nNA\n\n\n1989\nWatkins integra los metodos de aprendizaje (MA)\nWatkins, C. J. C. H. (1989). Learning from Delayed Rewards. Ph.D. thesis, Cambridge University.\n\n\n1996\nAparece el termino \"Neurodynamic Programming\"\nBertsekas, D. P., Tsitsiklis, J. N. (1989). Parallel and Distributed Computation: Numerical Methods. Prentice-Hall, Englewood Cliffs, NJ\n\n\n2003\n\"Reinforcement Learning\" en economí\nNA\n\n\n2012\nVision general del Reinforcement Learning y Juegos\nNA\n\n\nNA\nNA\nNA\n\n\n\nSource: Sutton and Barto (2018)\n\n\n\n\n\n\n\n\n\n## Adjust some defaults\ncolumn$color &lt;- \"black\"\ncolumn$size  &lt;- 15\ncolumn$background_color &lt;- \"pink\"\ncolumn$source_info &lt;- \"Source: Sutton and Barto (2018)\"\n\n## Milestones timeline\nmilestones(datatable = data, styles = column)\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_text()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_segment()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 25 rows containing missing values or values outside the scale range\n(`geom_segment()`).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Tarea 1</span>"
    ]
  },
  {
    "objectID": "Tarea 2.html#exercise-4",
    "href": "Tarea 2.html#exercise-4",
    "title": "2  Tarea 1",
    "section": "2.4 Exercise 4",
    "text": "2.4 Exercise 4\nExercise 4 Consider the following consumption-saving problem with dynamics \\[x_{k+1}=(1+r)(x_k-a_k),\\hspace{0.5cm}k=0,1,...,N-1,\\] and utility function \\[\\beta^N(x_N)^{1-\\gamma}+\\sum_{k=0}^{N-1}\\beta^k(a_k)^{1-\\gamma}\\]. Show that the value functions of the DP algorithm take the form \\[J_k(x)=A_k\\beta^kx^{1-\\gamma},\\] where \\(A_N=1\\) and for \\(k=N-1,...,0,\\) \\[A_k=\\left[1+((1+r)\\beta A_{k+1})^{\\frac{1}{\\gamma}} \\right]^{\\gamma}\\] Show also that the optimal policies are \\(h_k(x)=A_k^{-1/\\gamma} x,\\) for \\(k=N-1,\\ldots,0\\).\nPrueba Procedemos por inducción. Primero comprobamos que se cumple para \\(n=N-1\\), entonces como \\(J_{N}(x)=\\beta^{N}(x_N)^{1-\\gamma}\\), tenemos que\n\\[J_{N-1}=\\min_{a\\in A(x)}\\{\\beta^{N-1}(a)^{1-\\gamma} + \\beta^{N}(1+r)^{1-\\gamma}(x-a)^{1-\\gamma}\\},\\] tomando la derivada con respecto a \\(a\\) e igualando a cero. \\[(1-\\gamma)\\beta^{N-1}a^{-\\gamma}- \\beta^{N}(1+r)^{1-\\gamma}(x-a)^{-\\gamma}\\] \\[(1-\\gamma)\\beta^{N-1}a^{-\\gamma}-\\beta(1+r)^{1-\\gamma}(x-a)^{-\\gamma}=0\\] entonces \\[\\left(\\cfrac{x-a}{a}\\right)^\\gamma=\\beta(1+r)^{1-\\gamma}\\implies \\cfrac{x-a}{a}=[\\beta(1+r)^{1-\\gamma}]^{\\frac{1}{\\gamma}}\\implies a=\\dfrac{x}{[\\beta(1+r)^{1-\\gamma}]^{\\frac{1}{\\gamma}}+1}-\\] Sea \\(a_0\\) es punto donde se alcanza el mínimo, por tanto \\[J_{N-1}(x)=\\beta^{N-1}(a_0)^{1-\\gamma}+\\beta^N(1+r)^{1-\\gamma}(x-a_0)^{1-\\gamma}.\\] Desarrollando \\[J_{N-1}(x)=\\dfrac{\\beta^{N-1}x^{1-\\gamma}}{([\\beta(1+r)^{1-\\gamma}]^{\\frac{1}{\\gamma}}+1)^{1-\\gamma}}+\\beta^N(1+r)^{1-\\gamma}\\left[\\dfrac{x[\\beta(1+r)^{1-\\gamma}]^{\\frac{1}{\\gamma}}}{\\beta(1+r)^{1-\\gamma}]^{\\frac{1}{\\gamma}}+1}\\right]^{1-\\gamma}\\] \\[J_{N-1}(x)=\\beta^{N-1}x^{1-\\gamma}\\left[[\\beta(1+r)^{1-\\gamma}]^{\\frac{1}{\\gamma}}+1\\right]^{\\gamma}\\] \\[J_{N-1}(x)=A_{N-1}\\beta^{N-1}x^{1-\\gamma}\\] con \\[A_{N-1}=\\left(1+((1+r)^{1-\\gamma}\\beta)^{\\frac{1}{\\gamma}}\\right)^{\\gamma}\\] Ahora, supongamos que es válido para \\(n=k+1\\), 4 \\[J_{k+1}(x)=A_{k+1}\\beta^{k+1}x^{1-\\gamma}\\]. De aquí \\[J_k(x)=\\min_{a\\in (0,x)}\\left\\{\\beta^ka^{1-\\gamma}+A_{k+1}\\beta^{k+1}(1+r)^{1-\\gamma}(x-a)^{1-\\gamma} \\right\\}\\] Encontrando el punto mínimo \\[(1-\\gamma)\\beta^ka^{-\\gamma}-A_{k+1}\\beta^{k+1}(1+\\gamma)^{1-\\gamma}(1-\\gamma)(x-a)^{-\\gamma}=0\\] \\[(1-\\gamma)\\beta^k\\left[ a^{-\\gamma}-A_{k+1}\\beta(1+r)^{1-\\gamma}(x-a)^{-\\gamma}  \\right]=0\\] \\[a^{-\\gamma}-A_{k+1}\\beta(1+r)^{1-\\gamma}(x-a)^{-\\gamma}=0\\] \\[\\left(\\dfrac{x-a}{a}\\right)^{\\gamma}=A_{k+1}\\beta(1+\\gamma)^{1-\\gamma}\\] \\[a=\\dfrac{x}{\\left[ A_{k+1}\\beta(1+r)^{1-\\gamma}  \\right]^{\\frac{1}{\\gamma}}+1}\\] De igual forma sea el punto mínimo \\(a_0\\), se tiene \\[J_k(x)=\\dfrac{\\beta^kx^{1-\\gamma}}{\\left[\\left[ A_{k+1}\\beta(1+r)^{1-\\gamma}  \\right]^{\\frac{1}{\\gamma}}+1\\right]^{1-\\gamma}}+\\dfrac{A_{k+1}\\beta^{k+1}(1+r)^{1-\\gamma}\\left(x[A_{k+1}\\beta(1+r)^{1-\\gamma}]^{\\frac{1}{\\gamma}}\\right)^{1-\\gamma}}{\\left[\\left[ A_{k+1}\\beta(1+r)^{1-\\gamma}  \\right]^{\\frac{1}{\\gamma}}+1\\right]^{1-\\gamma}}\\] Simplificando se concluye que \\[\\dfrac{\\beta^kx^{1-\\gamma}\\left( 1+[A_{k+1}\\beta(1+r)^{1-\\gamma}]^{\\frac{1}{\\gamma}}  \\right)}{\\left(1+[A_{k+1}\\beta(1+r)^{1-\\gamma}]^{\\frac{1}{\\gamma}}  \\right)^{1-\\gamma}}=\\beta^kx^{1-\\gamma}A_k\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Tarea 1</span>"
    ]
  },
  {
    "objectID": "Tarea 2.html#exercise-5",
    "href": "Tarea 2.html#exercise-5",
    "title": "2  Tarea 1",
    "section": "2.5 Exercise 5",
    "text": "2.5 Exercise 5\nExercise 5 Consider now the infinite–horizon version of the above consumption–saving problem.\n\nWrite down the associated Bellman equation.\nArgue why a solution to the Bellman equation should be of the form \\[v(x)=cx^{1-\\gamma}\\], where \\(c\\) is constant. Find the constant and the stationary optimal policy.\n\nPrueba Sea \\[cx^{1-\\gamma}=\\min\\left\\{ a^{1-\\gamma}+\\beta c(1+r)^{1-\\gamma}(x-a)^{1-\\gamma} \\right\\}\\] Calculando el mínimo \\[(1-\\gamma)a^{-\\gamma}-\\beta c(1+r)^{1-\\gamma}(1-\\gamma)(x-a)^{-\\gamma}=0\\] \\[(1-\\gamma)\\left[ a^{-\\gamma}-\\beta c(1+r)^{1-\\gamma}(x-a)^{-\\gamma}\\right]=0\\] \\[a^{-\\gamma}-\\beta c(1+r)^{1-\\gamma}(x-a)^{-\\gamma}=0\\] \\[\\left(\\dfrac{x-a}{a}\\right)^{\\gamma}=\\beta c(1+r)^{1-\\gamma}\\] \\[x=a\\left[\\beta c(1+r)^{1-\\gamma} \\right]^{\\frac{1}{\\gamma}}+a\\] \\[a_0=a=\\dfrac{x}{\\left[\\beta c(1+r)^{1-\\gamma} \\right]^{\\frac{1}{\\gamma}}+1}\\] Sustitimos \\(a_0\\) \\[cx^{1-\\gamma}=\\dfrac{x^{1-\\gamma}+\\beta c(1+r)^{1-\\gamma}x^{1-\\gamma}\\left[ (\\beta c (1+r)^{1-\\gamma})^{\\frac{1}{\\gamma}} \\right]^{1-\\gamma}}{\\left[ (\\beta c (1+r)^{1-\\gamma})^{\\frac{1}{\\gamma}} +1\\right]^{1-\\gamma}}\\] \\[cx^{1-\\gamma}=x^{1-\\gamma}\\left[1+[\\beta c (1+r)^{1-\\gamma}]^{\\frac{1}{\\gamma}}\\right]^{\\gamma}\\] así, \\[cx^{1-\\gamma}=x^{1-\\gamma}\\left[1+[\\beta c(1+r)^{1-\\gamma}]^{\\frac{1}{\\gamma}}\\right]^{\\gamma}\\] \\[c=\\left[1+[\\beta c(1+r)^{1-\\gamma}]^{\\frac{1}{\\gamma}}\\right]^{\\gamma}\\] \\[c^{\\frac{1}{\\gamma}}=1+[\\beta c(1+r)^{1-\\gamma}]^{\\frac{1}{\\gamma}}\\] \\[c^{\\frac{1}{\\gamma}}=\\left[1-\\beta^{\\frac{1}{\\gamma}}(1+r)^{\\frac{1-\\gamma}{\\gamma}}\\right]\\] \\[c^{\\frac{1}{\\gamma}}=\\dfrac{1}{1-\\beta^{\\frac{1}{\\gamma}}(1+r)^{\\frac{1-\\gamma}{\\gamma}}}\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Tarea 1</span>"
    ]
  },
  {
    "objectID": "Tarea 2.html#exercise-6",
    "href": "Tarea 2.html#exercise-6",
    "title": "2  Tarea 1",
    "section": "2.6 Exercise 6",
    "text": "2.6 Exercise 6\nExercise 6 Let \\(\\{\\xi_k\\}\\) be a sequence of iid random variables such that \\(E[\\xi]=0\\) and \\(E[\\xi^2]=d\\). Consider the dynamics \\[x_{k+1}=x_k+a_k+\\xi_k,\\hspace{0.5cm}k=0,1,2,...,\\] and the discounted cost \\[E\\sum \\beta^k(a_k^2+x_k^2).\\] i. Write down the associated Bellman equation.\n\nConjecture that the solution to the Bellman equation takes the form \\(v(x)=ax^2+b\\), where \\(a\\) and \\(b\\) are constant.\nDetermine the constants \\(a\\) and \\(b\\).\nConjecture that the solution to the Bellman equation takes the form \\(v(x)=ax^2+b\\), where \\(a\\) y \\(b\\) are constant. Determine the constants \\(a\\) and \\(b\\). Prueba Sea \\(A=a\\) y \\(B=b\\), entonces \\[Ax^2+B=\\min_{a\\in A(x)}\\{a^2+x^2+\\beta E[A(x+a+\\xi)^2+B]\\}\\] \\[Ax^2+B=\\min_{a\\in A(x)}\\{a^2+x^2+\\beta AE[(x+a+\\xi)^2]+\\beta B\\}\\] \\[=\\min_{a\\in A(x)}\\{a^2+x^2+A\\beta E[x^2+2ax+a^2+2(x+a)\\xi+\\xi^2]+\\beta B\\}\\] \\[=\\min_{a\\in A(x)}\\{a^2+x^2+A\\beta x^2+2axA\\beta+A\\beta a^2+A\\beta d+\\beta B\\}\\] Encontrando el mínimo con la derivada \\[2a+2xA\\beta+2A\\beta a=0\\] entonces, \\[a=\\dfrac{-xA\\beta}{1+A\\beta}\\] así \\[Ax^2+B=\\dfrac{(xA\\beta)^2}{(a+A\\beta)^2}+x^2+\\beta E\\left[A\\left(\\dfrac{x}{1+A\\beta}+\\xi \\right)^2 \\right]+\\beta B\\] \\[=\\dfrac{x^2A^2\\beta^2}{(1+A\\beta)^2}+x^2+A\\beta E\\left[ \\dfrac{x^2}{(1+A\\beta)^2}+\\dfrac{2x\\xi}{1+A\\beta}+\\xi^2\\right]+\\beta B\\]\n\n\\[=\\dfrac{x^2A\\beta(1+A\\beta)}{(1+A\\beta)^2}+x^2+A\\beta d+\\beta B\\] \\[=x^2\\left(1+\\dfrac{A\\beta}{1+A\\beta}\\right)+A\\beta d+\\beta B\\] Por lo que \\[A=1+\\dfrac{A\\beta}{1+A\\beta},\\hspace{0.5cm}B=\\dfrac{A\\beta d}{1-\\beta}\\] De esta forma \\[A=\\dfrac{1+2A\\beta}{1+A\\beta}\\] \\[A^2\\beta+A(1-2\\beta)-1=0\\] Obteniendo las soluciones \\[A=\\dfrac{-1+2\\beta\\pm \\sqrt{4\\beta^2+1}}{2\\beta}\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Tarea 1</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "3  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "References"
    ]
  }
]