```{r}
library(bibtex)
## Activate the Core Packages
library(tidyverse) ## Brings in a core of useful functions
```

```{r}
library(gt)        ## Tables
## Specific packages
library(milestones)
## Initialize defaults
## Initialize defaults
column <- lolli_styles()

data <- read_csv(col_names=TRUE, show_col_types=FALSE, file='rl_time_line.csv')
```

```{r}
## Sort the table by date
data <- data |>
  arrange(date)

## Build a table
gt(data) |>
  #cols_hide(columns = event) |>
  tab_style(cell_text(v_align = "top"),
            locations = cells_body(columns = date)) |>
  tab_source_note(source_note = "Source: Sutton and Barto (2018)") 
```

```{r}
## Adjust some defaults
column$color <- "orange"
column$size  <- 15
column$source_info <- "Source: Sutton and Barto (2018)"

## Milestones timeline
milestones(datatable = data, styles = column)
```

## EJERCICIO 1
En el aprendizaje reforzado un agente aprende a tomar decisiones (acciones) a través de la interacción con su entorno y recibiendo recompensas o castigos en función de las mismas, a diferencia del aprendizaje supervisado, ya que en este tipo de aprendizaje automático, un modelo se entrena  utilizando un conjunto de datos que incluye tanto las entradas como las salidas correspondientes (etiquetas). es decir, consiste en aprender  a  partir  
de  un  conjunto  de  ejemplos ya etiquetados y proporcionados  por  un  supervisor  externo  con  conocimientos. Por lo que en este tipo de aprendizaje Cada  ejemplo describe una  situación especifica, y además existe una etiqueta que indica la acción adecuada que el sistema debe tomar en  esa  situación,    El  objetivo  de  este  tipo  de  aprendizaje  es  que  el  
sistema  generalice  sus  respuestas  para  que  actúe  correctamente  en  situaciones  que  no  están  presentes  en  el  conjunto  de  entrenamiento. Por otra parte El  aprendizaje  por  refuerzo  también  es  diferente  de  lo  que  los  investigadores  del  aprendizaje  automático  llaman  aprendizaje  no  supervisado, que  generalmente  consiste  en  
encontrar  estructuras  ocultas  en  conjuntos  de  datos  no  etiquetados y Aunque en parte el aprendizaje por refuerzo es un tipo de aprendizaje no supervisado, en realidad  este se centra  mas que nada en maximizar una recompensa en lugar de buscar patrones ocultos en los datos.

## EJERCICIO 2

 es posible pensar que dicha expresión es una función con la cual se mide el desempeño del sistema bajo diferentes políticas de control dado el estado inicial, es decir, nos ayuda a identificar que acciones fueron buenas y cuales fueron malas, además dicha expresión nos da el valor esperado de cuanta recompensa obtendremos en un futuro al elegir dicha politica dado un estado inicial. Por otra parte, el factor de descuento en la expresión, nos ayuda a comparar las recompensas futuras con las recompensas inmediatas, basicamente nos dice que tan a favor estamos de obtener una recompensa en el estado actual frente a un futuro lejano








## APD






del algoritmo de la programación dinámica se sigue que para este caso particular
_
$J_{N}(x)=\beta^{N}(x_N)^{1-\gamma}$

luego, para $k= N-1$
$$J_{N-1}=\min_{a\in A(x)}\{\beta^{N-1}(a)^{1-\gamma} + \beta^{N}(1+r)^{1-\gamma}(x-a)^{1-\gamma}\}$$
derivando con respecto a $a$ obtenemos 
$$(1-\gamma)\beta^{N-1}a^{-\gamma}- \beta^{N}(1+r)^{1-\gamma}(x-a)^{-\gamma}$$}
depués igualando a cero 
$$(1-\gamma)\beta^{N-1}\[a^{-\gamma}-\beta(1+r)^{1-\gamma}(x-a)^{-\gamma}\]=0$$
entonces 
$$(\cfrac{x-a}{a})^\gamma=\beta(1+r)^{1-\gamma}$$
$$\cfrac{x-a}{a}=[\beta(1+r)^{1-\gamma}]^{\frac{1}{\gamma}}$$

